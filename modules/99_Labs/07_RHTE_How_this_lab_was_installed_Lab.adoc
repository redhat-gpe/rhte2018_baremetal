:sectnums!:
:hardbreaks:
:scrollbar:
:data-uri:
:toc2:
:toc3:
:showdetailed:
:imagesdir: ./images


== Steps and Templates used to deploy the RedHat OpenStack environment for the Lab

=== OpenStack overcloud installation

In this section, you may review the templates used to deploy the OverCloud

Red Hat OpenStack Platform version 13 was installed in this lab.

. Parameters and Templates used for the OverCloud deployment
[%nowrap]
----
#!/bin/bash

openstack overcloud deploy \
--stack overcloud \
--templates /usr/share/openstack-tripleo-heat-templates \
--ntp-server 0.rhel.pool.ntp.org \
-e /usr/share/openstack-tripleo-heat-templates/environments/hyperconverged-ceph.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-rgw.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/cinder-backup.yaml  \
-e /home/stack/templates/environments/overcloud_images.yaml \
-e /home/stack/templates/environments/network-environment.yaml \
-e /home/stack/templates/environments/network-isolation.yaml \
-e /home/stack/templates/environments/node-info.yaml \
-e /home/stack/templates/environments/custom-config.yaml \
-e /home/stack/templates/environments/storage-ceph-hyperconverged-environment.yaml \
-e /home/stack/templates/environments/firstboot.yaml \
-e /home/stack/templates/environments/fix-nova-reserved-host-memory.yaml \
-e /home/stack/templates/environments/dns-network-config.yaml \
-e /home/stack/templates/environments/ironic.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/services/ironic.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/services-docker/octavia.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/services/barbican.yaml \
-e /usr/share/openstack-tripleo-heat-templates/environments/barbican-backend-simple-crypto.yaml  \
-e /usr/share/openstack-tripleo-heat-templates/environments/disable-telemetry.yaml

----

==== Configure the Hyper-Converged Infrastructure (HCI)

In HCI, both the storage area network and the underlying storage abstractions are implemented virtually in software (at or via the hypervisor) rather than physically, in hardware.

Four default environment templates were used to configure the Hyper-Converged Infrastructure.

* `/usr/share/openstack-tripleo-heat-templates/environments/hyperconverged-ceph.yaml`: Enable the Ceph in HCI mode will enable the services in controller and compute nodes.
* `/usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-rgw.yaml`: Enable the  Rados Gateway to use the CEPH Storage as a Object Storage provider.
* `/usr/share/openstack-tripleo-heat-templates/environments/ceph-ansible/ceph-ansible.yaml`: Uses `ceph-ansible` playbook to deploy the Ceph Cluster inside the overcloud nodes.
* `/usr/share/openstack-tripleo-heat-templates/environments/cinder-backup.yaml`: Configure Ceph RGW as Cinder Backup target

The custom configuration for CEPH used in the lab is defined in the file `storage-ceph-hyperconverged-environment.yaml` inside the `/home/stack/templates/environments/` directory.

. Review content of the file storage-ceph-hyperconverged-environment.yaml
[%nowrap]
----
resource_registry:
  # StorageMgmt
  OS::TripleO::Network::StorageMgmt: /usr/share/openstack-tripleo-heat-templates/network/storage_mgmt.yaml
  OS::TripleO::Network::External: /usr/share/openstack-tripleo-heat-templates/network/external.yaml
  OS::TripleO::Network::Ports::StorageMgmtVipPort: /usr/share/openstack-tripleo-heat-templates/network/ports/storage_mgmt.yaml
  OS::TripleO::Controller::Ports::StorageMgmtPort: /usr/share/openstack-tripleo-heat-templates/network/ports/storage_mgmt.yaml
  OS::TripleO::Compute::Ports::StorageMgmtPort: /usr/share/openstack-tripleo-heat-templates/network/ports/storage_mgmt.yaml
  OS::TripleO::Compute::Ports::ExternalPort: /usr/share/openstack-tripleo-heat-templates/network/ports/external.yaml

parameter_defaults:
  CephConfigOverrides:
    osd_pool_default_size: 1
    osd_pool_default_min_size: 1
    mon_max_pg_per_osd: 400
  CephAnsibleDisksConfig:
    osd_scenario: collocated
    devices:
      - /dev/vdb
      - /dev/vdc
----

Following parameters are defined in the template:

* The Storage Management network configuration.
* The Storage Management port configuration for controller and compute.
* Ceph parameters to be configured in `ceph.conf` files

.Parameters
[cols="1,2",options="header",caption="",options="nowrap"]
|===
| Parameter |Description
| `osd_pool_default_size` | Sets the number of replicas for objects in the pool.
| `osd_pool_default_min_size` | Sets the minimum number of written replicas for objects in the pool in order to acknowledge a write operation to the client.
| `mon_max_pg_per_osd` | Issue a HEALTH_WARN in cluster log if the average number of PGs per (in) OSD is above this number.
|===
* Configuration for the ceph-ansible playbook, where the disks are defined and the scenario (collocated).

In this lab two disks are used (/dev/vdb and /dev/vdc) for OSD but not disk used for journal.

==== Ironic

. Ironic deployment templates

The OverCloud Ironic Services were deployed using the template `/usr/share/openstack-tripleo-heat-templates/environments/services/ironic.yaml`.

You may find the following containerized services as part of the Ironic deployment

.Containers
[cols="1,2",options="header",caption="",options="nowrap"]
|===
| Container |Description
| `ironic_api` | Takes sanitized API commands from the API controller and performs the actions necessary to fulfill the API request.
| `ironic_conductor` |  Adds/edits/deletes nodes; powers on/off nodes with IPMI or other vendor-specific protocol; provisions/deploys/cleans bare metal nodes.
| `ironic_pxe_http` | Serves the PXE files via http.
| `ironic_pxe_tftp` | Services the PXE files via tftp.
| `nova_compute` | Runs the nova compute integrated with Ironic.
|===

A `ironic custom` template, `/home/stack/templates/environments/ironic.yaml` used in the `OverCloud` deployment to provide the custom parameters.
[source,yaml]
----
parameter_defaults:

    NovaSchedulerDefaultFilters:
        - RetryFilter
        - AggregateInstanceExtraSpecsFilter
        - AvailabilityZoneFilter
        - RamFilter
        - DiskFilter
        - ComputeFilter
        - ComputeCapabilitiesFilter
        - ImagePropertiesFilter

    IronicCleaningDiskErase: metadata
----

==== Octavia

. Octavia deployment Templates

The OpenStack loadbalancer service  `Octavia` is deployed using the template `/usr/share/openstack-tripleo-heat-templates/environments/services-docker/octavia.yaml`.
 
You may review the Containerized components deployed as part of Octavia service.

.Containers
[cols="1,2",options="header",caption="",options="nowrap"]
|===
| Container |Description
| `octavia_worker` | Takes sanitized API commands from the API controller and performs the actions necessary to fulfill the API request.
| `octavia_api` |  It takes API requests, performs simple sanitizing on them, and ships them off to the controller worker over the Oslo messaging bus.
| `octavia_health_manager` | Monitors individual amphorae to ensure they are up and running, and otherwise healthy. It also handles failover events if amphorae fail unexpectedly.
| `octavia_housekeeping` | Cleans up stale (deleted) database records, manages the spares pool, and manages amphora certificate rotation.
|===

==== Barbican

. Barbican deployment Templates

The template used to deploy iOctavia is on the path `/usr/share/openstack-tripleo-heat-templates/environments/services/barbican.yaml` to enable the container for the API and
`/usr/share/openstack-tripleo-heat-templates/environments/barbican-backend-simple-crypto.yaml` to enable a simple crypto algorithm.

The Barbican environment template deploy three containerized services

.Containers
[cols="1,2",options="header",caption="",options="nowrap"]
|===
| Container |Description
| `barbican_worker` | Processes tasks from the queue. Task components are similar to API resources in that they implement business logic and also interface with the datastore and follow on asynchronous tasks as needed.
| `barbican_keystone_listener` |  Barbican service should have its own dedicated notification queue so that it receives all of Keystone notifications.
| `barbican_api` | Handles incoming REST requests to Barbican. These nodes can interact with the database directly if the request can be completed synchronously (such as for GET requests), otherwise the queue supports asynchronous processing by worker nodes.
|===

==== Network configuration

. Network Environment Templates

Following custom templates used to define the network configurations for the OverCloud in the lab environment

* `/home/stack/templates/environments/network-environment.yaml`: Defines the network ranges and the VLAN IDs for Overcloud.
+
.File content
[%nowrap]
----
resource_registry:
  # NIC Configs for our roles
  OS::TripleO::Compute::Net::SoftwareConfig: ../nic-configs/compute.yaml
  OS::TripleO::Controller::Net::SoftwareConfig: ../nic-configs/controller.yaml

parameter_defaults:
  # Internal API used for private OpenStack Traffic
  InternalApiNetCidr: 172.17.0.0/24
  InternalApiAllocationPools: [{'start': '172.17.0.20', 'end': '172.17.0.200'}]
  InternalApiNetworkVlanID: 20

  # Tenant Network Traffic - will be used for VXLAN over VLAN
  TenantNetCidr: 172.16.0.0/24
  TenantAllocationPools: [{'start': '172.16.0.20', 'end': '172.16.0.200'}]
  TenantNetworkVlanID: 50

  # Public Storage Access - e.g. Nova/Glance <--> Ceph
  StorageNetCidr: 172.20.0.0/24
  StorageAllocationPools: [{'start': '172.20.0.20', 'end': '172.20.0.200'}]

  # Private Storage Access - i.e. Ceph background cluster/replication
  StorageMgmtNetCidr: 172.20.1.0/24
  StorageMgmtAllocationPools: [{'start': '172.20.1.20', 'end': '172.20.1.200'}]


  ExternalNetCidr: 10.0.0.0/24
  # Leave room for floating IPs in the External allocation pool (if required)
  ExternalAllocationPools: [{'start': '10.0.0.20', 'end': '10.0.0.200'}]
  ExternalNetworkVlanID: 10
  # Set to the router gateway on the external network
  ExternalInterfaceDefaultRoute: 10.0.0.1


  # CIDR subnet mask length for provisioning network
  ControlPlaneSubnetCidr: "24"
  # Gateway router for the provisioning network (or Undercloud IP)
  ControlPlaneDefaultRoute: 192.0.2.1
  # Generally the IP of the Undercloud
  EC2MetadataIp: 192.0.2.1

----
* `/home/stack/templates/environments/network-isolation.yaml`: Enable the creation of Neutron networks for isolated Overcloud traffic and configure the roles to assign ports (related to that role) on the networks.

+
.File content
[%nowrap]
----
resource_registry:
  OS::TripleO::Network::External: /usr/share/openstack-tripleo-heat-templates/network/external.yaml
  OS::TripleO::Network::InternalApi: /usr/share/openstack-tripleo-heat-templates/network/internal_api.yaml
  OS::TripleO::Network::Storage: /usr/share/openstack-tripleo-heat-templates/network/storage.yaml
  OS::TripleO::Network::Tenant: /usr/share/openstack-tripleo-heat-templates/network/tenant.yaml
  # Management network is optional and disabled by default.
  # To enable it, include environments/network-management.yaml
  OS::TripleO::Network::Management: /usr/share/openstack-tripleo-heat-templates/network/management.yaml

  # Port assignments for the VIPs
  OS::TripleO::Network::Ports::ExternalVipPort: /usr/share/openstack-tripleo-heat-templates/network/ports/external.yaml
  OS::TripleO::Network::Ports::InternalApiVipPort: /usr/share/openstack-tripleo-heat-templates/network/ports/internal_api.yaml
  OS::TripleO::Network::Ports::StorageVipPort: /usr/share/openstack-tripleo-heat-templates/network/ports/storage.yaml
  OS::TripleO::Network::Ports::RedisVipPort: /usr/share/openstack-tripleo-heat-templates/network/ports/vip.yaml

  # Port assignments for the controller role
  OS::TripleO::Controller::Ports::ExternalPort: /usr/share/openstack-tripleo-heat-templates/network/ports/external.yaml
  OS::TripleO::Controller::Ports::InternalApiPort: /usr/share/openstack-tripleo-heat-templates/network/ports/internal_api.yaml
  OS::TripleO::Controller::Ports::StoragePort: /usr/share/openstack-tripleo-heat-templates/network/ports/storage.yaml
  OS::TripleO::Controller::Ports::TenantPort: /usr/share/openstack-tripleo-heat-templates/network/ports/tenant.yaml
  OS::TripleO::Controller::Ports::ManagementPort: /usr/share/openstack-tripleo-heat-templates/network/ports/management.yaml

  # Port assignments for the compute role
  OS::TripleO::Compute::Ports::ExternalPort: /usr/share/openstack-tripleo-heat-templates/network/ports/external.yaml
  OS::TripleO::Compute::Ports::InternalApiPort: /usr/share/openstack-tripleo-heat-templates/network/ports/internal_api.yaml
  OS::TripleO::Compute::Ports::StoragePort: /usr/share/openstack-tripleo-heat-templates/network/ports/storage.yaml
  OS::TripleO::Compute::Ports::TenantPort: /usr/share/openstack-tripleo-heat-templates/network/ports/tenant.yaml
  OS::TripleO::Compute::Ports::ManagementPort: /usr/share/openstack-tripleo-heat-templates/network/ports/management.yaml
----

* `/home/stack/templates/nic-configs/compute.yaml` and `/home/stack/templates/nic-configs/controller.yaml`: Define the interfaces/bridge for the controller and the compute nodes and its attached networks.
+
.File content
[%nowrap]
----
heat_template_version: queens

parameters:
  ControlPlaneIp:
    default: ''
    description: IP address/subnet on the ctlplane network
    type: string
  ExternalIpSubnet:
    default: ''
    description: IP address/subnet on the external network
    type: string
  InternalApiIpSubnet:
    default: ''
    description: IP address/subnet on the internal_api network
    type: string
  StorageIpSubnet:
    default: ''
    description: IP address/subnet on the storage network
    type: string
  StorageMgmtIpSubnet:
    default: ''
    description: IP address/subnet on the storage_mgmt network
    type: string
  TenantIpSubnet:
    default: ''
    description: IP address/subnet on the tenant network
    type: string
  ManagementIpSubnet: # Only populated when including environments/network-management.yaml
    default: ''
    description: IP address/subnet on the management network
    type: string
  BondInterfaceOvsOptions:
    default: 'bond_mode=active-backup'
    description: The ovs_options string for the bond interface. Set things like
                 lacp=active and/or bond_mode=balance-slb using this option.
    type: string
  ExternalNetworkVlanID:
    default: 10
    description: Vlan ID for the external network traffic.
    type: number
  InternalApiNetworkVlanID:
    default: 20
    description: Vlan ID for the internal_api network traffic.
    type: number
  StorageNetworkVlanID:
    default: 30
    description: Vlan ID for the storage network traffic.
    type: number
  StorageMgmtNetworkVlanID:
    default: 40
    description: Vlan ID for the storage_mgmt network traffic.
    type: number
  TenantNetworkVlanID:
    default: 50
    description: Vlan ID for the tenant network traffic.
    type: number
  ManagementNetworkVlanID:
    default: 60
    description: Vlan ID for the management network traffic.
    type: number
  ExternalInterfaceDefaultRoute:
    default: '10.0.0.1'
    description: default route for the external network
    type: string
  ControlPlaneSubnetCidr: # Override this via parameter_defaults
    default: '24'
    description: The subnet CIDR of the control plane network.
    type: string
  ControlPlaneDefaultRoute: # Override this via parameter_defaults
    description: The default route of the control plane network.
    type: string
  DnsServers: # Override this via parameter_defaults
    default: []
    description: A list of DNS servers (2 max for some implementations) that will be added to resolv.conf.
    type: comma_delimited_list
  EC2MetadataIp: # Override this via parameter_defaults
    description: The IP address of the EC2 metadata server.
    type: string

resources:
  OsNetConfigImpl:
    type: OS::Heat::SoftwareConfig
    properties:
      group: script
      config:
        str_replace:
          template:
            get_file: /usr/share/openstack-tripleo-heat-templates/network/scripts/run-os-net-config.sh
            params:
              $network_config:
                network_config:
                - type: ovs_bridge
                  name: br-baremetal
                  use_dhcp: false
                  members:
                    -
                      type: interface
                      name: nic5

                - type: interface
                  name: nic1
                  mtu: 1500
                  use_dhcp: false
                  addresses:
                  - ip_netmask:
                      list_join:
                      - /
                      - - {get_param: ControlPlaneIp}
                        - {get_param: ControlPlaneSubnetCidr}
              -
                type: ovs_bridge
                name: bridge_name
                use_dhcp: false
                members:
                  -
                    type: interface
                    name: nic2

              -
                type: vlan
                vlan_id: {get_param: ExternalNetworkVlanID}
                device: br-ex
                addresses:
                  -
                    ip_netmask: {get_param: ExternalIpSubnet}
                routes:
                - default: true
                  next_hop: {get_param: ExternalInterfaceDefaultRoute}

              -
                type: vlan
                vlan_id: {get_param: InternalApiNetworkVlanID}
                device: br-ex
                addresses:
                  -
                    ip_netmask: {get_param: InternalApiIpSubnet}
              -
                type: vlan
                vlan_id: {get_param: TenantNetworkVlanID}
                device: br-ex
                addresses:
                  -
                    ip_netmask: {get_param: TenantIpSubnet}

              -
                type: ovs_bridge
                name: br-storage
                use_dhcp: false
                mtu: 1500
                addresses:
                  - ip_netmask: {get_param: StorageIpSubnet}
                members:
                  -
                    type: interface
                    name: nic3

              -
                type: ovs_bridge
                name: br-storage-mgmt
                use_dhcp: false
                mtu: 9000
                addresses:
                  - ip_netmask: {get_param: StorageMgmtIpSubnet}
                members:
                  -
                    type: interface
                    name: nic4

outputs:
  OS::stack_id:
    description: The OsNetConfigImpl resource.
    value: {get_resource: OsNetConfigImpl}
----
+
.Network Interfaces in the overcloud nodes
[cols="1,1,1,2",options="header",caption="",options="nowrap"]
|===
| Interface | OpenStack Name |OVS Bridge |Description
| `eth0` | `nic1` | N/D | Connected to the Provisioning network (DHCP from 192.0.2.0/24)
| `eth1` | `nic2` | br-ex | Connected to a trunk (VLANs for Internal API, External and Tenant)
| `eth2` | `nic3` | br-storage | Connected to the Storage Network.
| `eth3` | `nic4` | br-storage-mgmt | Connected to the Storage Management Network.
|===

==== Custom configuration templates

. Additional custom templates used to provision the lab

* `/home/stack/templates/environments/node-info.yaml`: Specifies the number of nodes per role and their flavors.
+
.File content
[%nowrap]
----
parameter_defaults:
  ControllerCount: 3
  ComputeCount: 2
  OvercloudControllerFlavor: control
  OvercloudComputeFlavor: compute
----

* `/home/stack/templates/heat/firstboot.yaml`: Defines the user data to be used during the first boot on all the overcloud nodes.
+
.File content
[%nowrap]
----
heat_template_version: 2014-10-16

description: >
  Set root password

resources:
  userdata:
    type: OS::Heat::MultipartMime
    properties:
      parts:
      - config: {get_resource: set_pass}

  set_pass:
    type: OS::Heat::SoftwareConfig
    properties:
      config: |
        #!/bin/bash
        echo 'r3dh4t1!' | passwd --stdin root

outputs:
  OS::stack_id:
    value: {get_resource: userdata}
----

* `/home/stack/templates/environments/firstboot.yaml`: Defines the `userdata` resource, which sets the `root` password
+
.File content
[%nowrap]
----
resource_registry:
  OS::TripleO::NodeUserData: ../heat/firstboot.yaml
----


* `/home/stack/templates/environments/custom-config.yaml`: Misc configuration for the Overcloud
+
.File content
[%nowrap]
----
parameter_defaults:
  OctaviaAmphoraSshKeyFile: /tmp/id_rsa.pub (1)
  BarbicanSimpleCryptoGlobalDefault: true (2)
  TimeZone: 'America/New_York' (3)
  ServiceNetMap: (4)
    KeystoneAdminApiNetwork: external
----

[NOTE]

. The file specified by the parameter `OctaviaAmphoraSshKeyFile` must be readable by the `mistral` user on the undercloud server.
. Enable the `Simple Crypto` globally.
. Defines the timezone.
. Change the network for the `Keystone` admin network.

[IMPORTANT]
The endpoint for the Keystone Admin was moved to the external due to the integration with OpenShift.

. DNS Configuration Templates

* `/home/stack/templates/environments/dns-network-config.yaml`: Template to configure the DNS for the Overcloud Nodes
+
.File content
[%nowrap]
----
parameter_defaults:
  CloudName: openstack.example.com
  CloudDomain: example.com
  # Define the DNS servers (maximum 2) for the overcloud nodes
  DnsServers: ['192.0.2.254']
----

* `/home/stack/templates/environments/fix-nova-reserved-host-memory.yaml`: Custom parameters to over ride the lab environment limitation
+
.File content
[%nowrap]
----
parameter_defaults:
  NovaReservedHostMemory: 1024
----
+
[NOTE]
By default, the RedHat OpenStack Platform director reserves `4 GB` of the compute nodes' memory for host processes. This may not feasible for small scale deployments. The `NovaReservedHostMemory` parameter limits the reserved host memory to `1 GB`.


==== Disable metrics

The environment file `/usr/share/openstack-tripleo-heat-templates/environments/disable-telemetry.yaml` used to  to  disable the  ceilometer, gnocchi, aodh and panko services to restrict the OverCloud resource utilization.

=== Deploy a DNS instance to be used for OpenShift

. DNS Server Node

For the DNS VM installation was used the playbooks hosted on `https://github.com/tomassedovic/devns`.
This DNS VM server is possible to update dynamically the DNS using `nsupdate` and  is used by the OpenShift installation to generate the needed records.


Following steps were used for the  DNS Server installation:

. Configure file `/home/stack/devns/vars.yaml`
+
.File content
+
[%nowrap]
----
---
dns_domain: openshift.example.com

external_network: public

# openstack keypair list
key_name: openshift

# openstack image list
image: rhel7

# openstack flavor list
flavor: m1.small2

server_name: openshift-dns

dns_forwarders: ["8.8.8.8"]
----
+
. Install `shade` package for python
We need to install `shade` library as a `prerequisite` to install the `OpenShift` on OpenStack. `Shade` is a simple client library for interacting with OpenStack clouds.
+
[%nowrap]
----
(overcloud) [stack@undercloud ~]$ sudo yum install python2-shade
----
+
.Sample output
[%nowrap]
----
Loaded plugins: product-id, search-disabled-repos, subscription-manager
This system is not registered with an entitlement server. You can use subscription-manager to register.
Resolving Dependencies
--> Running transaction check
---> Package python2-shade.noarch 0:1.27.1-1.el7ost will be installed
--> Finished Dependency Resolution

Dependencies Resolved

==============================================================================================================================================================================================
 Package                                   Arch                               Version                                       Repository                                                   Size
==============================================================================================================================================================================================
Installing:
 python2-shade                             noarch                             1.27.1-1.el7ost                               rhel-7-server-openstack-13-rpms                             552 k

Transaction Summary
==============================================================================================================================================================================================
Install  1 Package

Total download size: 552 k
Installed size: 3.1 M
Is this ok [y/d/N]: y
Downloading packages:
python2-shade-1.27.1-1.el7ost.noarch.rpm                                                                                                                               | 552 kB  00:00:00
Running transaction check
Running transaction test
Transaction test succeeded
Running transaction
  Installing : python2-shade-1.27.1-1.el7ost.noarch                                                                                                                                       1/1
  Verifying  : python2-shade-1.27.1-1.el7ost.noarch                                                                                                                                       1/1

Installed:
  python2-shade.noarch 0:1.27.1-1.el7ost

Complete!
----

+
. Deploy the VMs using Ansible playbook
+
Once the `vars.yaml` is configured, we only need to call to the playbook `deploy.yaml` to create:

* Network with name `openshift-dns`
* Subnet with name `openshift-dns` with range `192.168.23.0/24`
* Router connected to the `public` network with name `openshift-dns`
* VM with name `openshift-dns`
+
.Run the `deploy.yaml` playbook
[%nowrap]
----
(overcloud) [stack@undercloud ~]$ cd devns/
(overcloud) [stack@undercloud ~]$ ansible-playbook --private-key ~/.ssh/id_rsa --user cloud-user deploy.yaml -e @vars.yaml
----
+
When the deployment finishes, it shows the `Floating IP`, `key algorithm` and `key secret`  used to update DNS dinamically.


=== OpenShift installation

. OpenShift Container Platform Installation

In this section, you may review the installation steps used to deploy OpenShift in the lab environment

OpenShift Container Platform 3.10 was installed on top of OpenStack using  the `openshift-ansible` playbooks. 


==== Review the OpenShift installation variables

The ansible inventory file  `/root/hosts` contains the `node` configuration and Parameters used for the `OpenShift` installation.

.File content
[%nowrap]
----
[masters]
ocp-master01.openshift.example.com
[etcd]
ocp-master01.openshift.example.com
[nodes]
ocp-master01.openshift.example.com openshift_ip=192.0.3.16 ansible_host=192.0.3.16 openshift_node_group_name='node-config-master'
ocp-infra01.openshift.example.com openshift_ip=192.0.3.22 ansible_host=192.0.3.22 openshift_node_group_name='node-config-infra'
ocp-node01.openshift.example.com openshift_ip=192.0.3.12 ansible_host=192.0.3.12 openshift_node_group_name='node-config-compute'
[new_nodes]
ocp-node02.openshift.example.com openshift_ip=192.0.3.14 ansible_host=192.0.3.14 openshift_node_group_name='node-config-compute'
[OSEv3:children]
masters
nodes
new_nodes
[OSEv3:vars]
ansible_user=cloud-user
ansible_become=yes
openshift_deployment_type=openshift-enterprise
openshift_release="3.10"
openshift_master_default_subdomain=apps.openshift.example.com
openshift_master_cluster_hostname=console.openshift.example.com
debug_level=2
openshift_disable_check=disk_availability,memory_availability,docker_storage,package_availability,package_version
openshift_additional_repos=[{'id': 'ose-repo', 'name': 'rhel-7-server-ose-3.10-rpms', 'baseurl': 'http://192.0.2.253/repos/rhel-7-server-ose-3.10-rpms', 'enabled': 1, 'gpgcheck': 0},{'id': 'rhel-7-server-rpms-repo', 'name': 'rhel-7-server-rpms', 'baseurl': 'http://192.0.2.253/repos/rhel-7-server-rpms', 'enabled': 1, 'gpgcheck': 0},{'id': 'rhel-7-server-extras-rpms-repo', 'name': 'rhel-7-server-extras-rpms', 'baseurl': 'http://192.0.2.253/repos/rhel-7-server-extras-rpms', 'enabled': 1, 'gpgcheck': 0},{'id': 'rhel-7-fast-datapath-rpms', 'name': 'rhel-7-fast-datapath-rpms', 'baseurl': 'http://192.0.2.253/repos/rhel-7-fast-datapath-rpms', 'enabled': 1, 'gpgcheck': 0}]
openshift_cloudprovider_kind=openstack
openshift_cloudprovider_openstack_auth_url="http://10.0.0.26:5000//v3"
openshift_cloudprovider_openstack_username="admin"
openshift_cloudprovider_openstack_password="Eu3xKG6UKpKvZReEFc7FKqCn6"
openshift_cloudprovider_openstack_tenant_name="admin"
openshift_cloudprovider_openstack_region="regionOne"
openshift_cloudprovider_openstack_domain_name="Default"
openshift_cloudprovider_openstack_blockstorage_version=v2
osm_default_node_selector='region=primary'
openshift_hosted_router_selector='node-role.kubernetes.io/infra=true'
os_sdn_network_plugin_name='redhat/openshift-ovs-multitenant'
openshift_enable_service_catalog=false
template_service_broker_install=false
openshift_ca_cert_expire_days=1825
openshift_node_cert_expire_days=730
openshift_master_cert_expire_days=730
etcd_ca_default_days=1825
----

==== OpenShift installation via Playbook

OpenShift is using the plabooks provided as `openshift-ansible` package for the deployment and administration. After installing the package `openshift-ansible` you could find the play books for variouse tasks under `/usr/share/ansible/openshift-ansible/playbooks/`

.To deploy `OpenShift cluster` you may run the playbook with the required `inventory` file (hosts) as the input.
[%nowrap]
----
[root@ocp-bastion ~]# ansible-playbook -i <inventory file path> /usr/share/ansible/openshift-ansible/playbooks/deploy_cluster.yml
----
